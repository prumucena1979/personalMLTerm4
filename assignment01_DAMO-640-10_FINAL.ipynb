{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas numpy scikit-learn matplotlib jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf2033",
   "metadata": {},
   "source": [
    "# DAMO-640-10 · Fall 2025  \n",
    "## Assignment 1 — Supervised Learning (Haberman’s Survival Dataset)\n",
    "\n",
    "**Student Name:** [Fabio dos Santos Prumucena]  \n",
    "**Course:** Advanced Data Analytics (DAMO-640)  \n",
    "**Instructor:** [Ali El-Sharif]   \n",
    "**Institution:** University of Niagara Falls Canada  \n",
    "**Public Repository:** [https://github.com/prumucena1979/personalMLTerm4]\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "This notebook presents the full implementation of a supervised learning pipeline applied to the *Haberman’s Survival Dataset* as part of Assignment 1. \n",
    "The objective is to build, tune, and evaluate predictive models capable of estimating post-surgery survival outcomes in breast cancer patients. \n",
    "The following tasks align directly with the assignment requirements and modules 1–4 of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c67f6",
   "metadata": {},
   "source": [
    "#All imports necessary were concentrated in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_curve, roc_auc_score, classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1acb189",
   "metadata": {},
   "source": [
    "## Task 1 — Data Loading and Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this first step, the dataset is loaded and its basic characteristics are explored. The four columns—**age**, **operation_year**, **axillary_nodes**, and **survival_status**—are assigned according to the UCI repository description. Summary statistics and the class distribution are analyzed to understand the balance between survivors and non‑survivors.\n",
    "\n",
    "Potential data issues such as missing values or outliers are examined to ensure the integrity of subsequent analysis. This step lays the foundation for all preprocessing and modeling phases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a52c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 — Data Loading & EDA \n",
    "\n",
    "# Dataset URL from the assignment\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\"\n",
    "\n",
    "# Load the CSV directly from the URL (no header) and assign column names\n",
    "df = pd.read_csv(\n",
    "    url,\n",
    "    header=None,\n",
    "    names=[\"age\", \"operation_year\", \"axillary_nodes\", \"survival_status\"]\n",
    ")\n",
    "\n",
    "print(\" Dataset loaded successfully from UCI Repository.\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# Display basic EDA\n",
    "display(df.head())\n",
    "display(df.describe())\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nClass counts:\")\n",
    "print(df[\"survival_status\"].value_counts())\n",
    "print(\"\\nClass ratios:\")\n",
    "print(df[\"survival_status\"].value_counts(normalize=True))\n",
    "\n",
    "# Check for missing or invalid values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "invalid_years = (df[\"operation_year\"] < 1958) | (df[\"operation_year\"] > 1969)\n",
    "invalid_nodes = df[\"axillary_nodes\"] < 0\n",
    "\n",
    "print(f\"\\nInvalid operation_year rows: {invalid_years.sum()}\")\n",
    "print(f\"Invalid axillary_nodes rows: {invalid_nodes.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4891065",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset information:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064b6511",
   "metadata": {},
   "source": [
    "### Sub‑Task 1.1 — Validation of Input Variables\n",
    "\n",
    "At this stage, the dataset is checked for invalid or inconsistent entries (e.g., operation years outside 1958–1969, or negative node counts). Detecting and handling such anomalies ensures the model learns from accurate and representative data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb0c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Records with invalid operation years (outside 1958-1969):\")\n",
    "print(df[~df['operation_year'].between(1958, 1969)])\n",
    "print(\"\\nRecords with negative axillary nodes:\")\n",
    "print(df[df['axillary_nodes'] < 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e595665",
   "metadata": {},
   "source": [
    "## Task 2 — Preprocessing and Feature Engineering\n",
    "\n",
    "The variable *survival_status* is converted from {1, 2} into a binary outcome {1 = survived ≥ 5 years, 0 = died < 5 years}. Data are then split into **training (75 %)** and **testing (25 %)** subsets, preserving class proportions when appropriate.\n",
    "\n",
    "Feature scaling and transformation techniques are applied when needed to improve model convergence and interpretability. The resulting datasets serve as clean inputs for the supervised learning algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compliance Merge — Task 2: Preprocessing (encoding, split 75/25 rs=42, scaling)\n",
    "\n",
    "# Encode target {1:1, 2:0}\n",
    "df = df.copy()\n",
    "df[\"label\"] = df[\"survival_status\"].map({1:1, 2:0})\n",
    "\n",
    "# Feature–target separation\n",
    "X = df[[\"age\",\"operation_year\",\"axillary_nodes\"]]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Hold-out split with stratify and fixed seed\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Optional inspection\n",
    "print(\"\\nBefore standardization (example):\")\n",
    "display(X_train.head())\n",
    "print(\"\\nAfter standardization (first 5 rows):\")\n",
    "print(pd.DataFrame(X_train_std, columns=X.columns).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ddbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Convert survival_status to binary\n",
    "# In the original dataset: 1 = survived ≥5 years, 2 = died <5 years\n",
    "df['label'] = df['survival_status'].map({1: 1, 2: 0})\n",
    "print(\"Binary label conversion completed.\")\n",
    "print(\"Label distribution:\")\n",
    "print(df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ Separate features (X) and label (y)\n",
    "X = df[['age', 'operation_year', 'axillary_nodes']].copy()\n",
    "y = df['label'].copy()\n",
    "print(\"Features (X) and labels (y) separated successfully.\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf36cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣ Split into train and test (75/25)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ") \n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e63a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "print(\"Feature standardization completed.\")\n",
    "print(f\"Training set mean: {X_train_std.mean(axis=0).round(4)}\")\n",
    "print(f\"Training set std: {X_train_std.std(axis=0).round(4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f2638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: show before/after to check standardization\n",
    "print(\"\\nBefore standardization (example):\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\nAfter standardization (first 5 rows):\")\n",
    "print(pd.DataFrame(X_train_std, columns=X.columns).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7c5d4",
   "metadata": {},
   "source": [
    "## Task 3 — Model Training and Dimensionality Reduction\n",
    "\n",
    "Multiple models are tested to determine which best captures survival patterns. Here, both **Decision Tree (DT)** and **Logistic Regression (LR)** classifiers are implemented, representing interpretable and probabilistic approaches respectively.\n",
    "\n",
    "Additionally, **Principal Component Analysis (PCA)** is optionally used to visualize data separability and assess whether dimensionality reduction improves model performance or interpretability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7336d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compliance Merge — Task 3: PCA (fit on train, CEV plot, ≥90% selection, transform both)\n",
    "\n",
    "# Fit PCA on standardized train\n",
    "pca_probe = PCA().fit(X_train_std)\n",
    "cum_var = np.cumsum(pca_probe.explained_variance_ratio_)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(cum_var)+1), cum_var, marker=\"o\")\n",
    "plt.axhline(0.90, linestyle=\"--\", label=\"90% threshold\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA — Cumulative Explained Variance\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Choose min components to reach ≥90%\n",
    "n_components_90 = int(np.argmax(cum_var >= 0.90) + 1)\n",
    "print(\"Explained variance per component:\", pca_probe.explained_variance_ratio_)\n",
    "print(\"Cumulative variance:\", cum_var)\n",
    "print(f\"→ Number of components needed for ≥90% variance: {n_components_90}\")\n",
    "\n",
    "# Apply PCA with chosen components\n",
    "pca = PCA(n_components=n_components_90, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "print(\"Transformed shapes:\", X_train_pca.shape, X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Fit PCA on standardized training set\n",
    "pca_probe = PCA().fit(X_train_std)\n",
    "print(\"PCA fitted on standardized training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06706d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ Calculate cumulative variance\n",
    "cum_var = np.cumsum(pca_probe.explained_variance_ratio_)\n",
    "print(\"Cumulative variance calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45eee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣ Determine minimum number of components with >= 90% variance\n",
    "n_components_90 = int(np.argmax(cum_var >= 0.90) + 1)\n",
    "print(\"Explained variance per component:\", pca_probe.explained_variance_ratio_)\n",
    "print(\"Cumulative variance:\", cum_var)\n",
    "print(f\"→ Number of components needed for ≥90% variance: {n_components_90}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99faf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ Plot cumulative variance\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(1, len(cum_var)+1), cum_var, marker='o')\n",
    "plt.axhline(0.90, color='r', linestyle='--', label='90% threshold')\n",
    "plt.title(\"PCA — Cumulative Explained Variance\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Variance\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5️⃣ Apply PCA with chosen number of components\n",
    "pca = PCA(n_components=n_components_90, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "print(\"Shape of transformed data:\")\n",
    "print(\"Train:\", X_train_pca.shape)\n",
    "print(\"Test:\", X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb24f16",
   "metadata": {},
   "source": [
    "### Sub‑Task 3.1 — Hyperparameter Tuning\n",
    "\n",
    "A systematic grid search is conducted for each algorithm to optimize performance. For Decision Trees, parameters such as `max_depth` and `min_samples_split` are tuned. For Logistic Regression, parameters including `C`, `penalty`, and `solver` are adjusted.\n",
    "\n",
    "The best combination of hyperparameters is selected based on **cross‑validation AUC**, ensuring that the model generalizes well to unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af3c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"Cross-validation strategy initialized: 5-fold stratified with shuffling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d93b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 1️⃣ Logistic Regression — test two values of C\n",
    "# -------------------------------------------------------------------\n",
    "print(\"Testing Logistic Regression with different C values...\")\n",
    "lr_results = []\n",
    "for C in [0.1, 1.0]:\n",
    "    lr = LogisticRegression(C=C, random_state=42, max_iter=200)\n",
    "    scores = cross_val_score(lr, X_train_pca, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    lr_results.append({\n",
    "        \"Model\": \"LogisticRegression\",\n",
    "        \"C\": C,\n",
    "        \"Mean_CV_Accuracy\": scores.mean(),\n",
    "        \"Std\": scores.std()\n",
    "    })\n",
    "\n",
    "lr_df = pd.DataFrame(lr_results)\n",
    "print(\"Logistic Regression cross-validation results:\")\n",
    "display(lr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2314c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best C value\n",
    "best_lr_row = lr_df.iloc[lr_df[\"Mean_CV_Accuracy\"].idxmax()]\n",
    "best_lr_C = best_lr_row[\"C\"]\n",
    "print(f\"✅ Best C for Logistic Regression: {best_lr_C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 2️⃣ Decision Tree — test two values of max_depth\n",
    "# -------------------------------------------------------------------\n",
    "print(\"Testing Decision Tree with different max_depth values...\")\n",
    "dt_results = []\n",
    "for depth in [3, None]:\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    scores = cross_val_score(dt, X_train_pca, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    dt_results.append({\n",
    "        \"Model\": \"DecisionTree\",\n",
    "        \"max_depth\": \"None\" if depth is None else depth,\n",
    "        \"Mean_CV_Accuracy\": scores.mean(),\n",
    "        \"Std\": scores.std()\n",
    "    })\n",
    "\n",
    "dt_df = pd.DataFrame(dt_results)\n",
    "print(\"Decision Tree cross-validation results:\")\n",
    "display(dt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f476f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best max_depth\n",
    "best_dt_row = dt_df.iloc[dt_df[\"Mean_CV_Accuracy\"].idxmax()]\n",
    "best_dt_depth = None if best_dt_row[\"max_depth\"] == \"None\" else int(best_dt_row[\"max_depth\"])\n",
    "print(f\"✅ Best max_depth for Decision Tree: {best_dt_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11238461",
   "metadata": {},
   "source": [
    "## Task 4 — Model Evaluation and Comparative Analysis\n",
    "\n",
    "The tuned models are evaluated on the hold‑out test set. The following metrics are computed and compared:\n",
    "\n",
    "- **AUC (Area Under the ROC Curve)** – indicator of discriminative ability  \n",
    "- **Accuracy, Precision, Recall, and F1‑Score** – for classification performance  \n",
    "- **Confusion Matrix** – for error distribution insights  \n",
    "\n",
    "The comparative results show that Logistic Regression significantly outperformed Decision Tree in both cross‑validation and final test AUC, suggesting that the linear, regularized model better captures the underlying patterns of survival outcomes.\n",
    "\n",
    "### Example Code Snippet — Comparing Tuned Models\n",
    "\n",
    "```python\n",
    "print(\"Decision Tree best params:\", dt_grid.best_params_)\n",
    "print(\"Logistic Regression best params:\", lr_grid.best_params_)\n",
    "\n",
    "print(f\"Decision Tree AUC (CV): {dt_grid.best_score_:.4f}\")\n",
    "print(f\"Logistic Regression AUC (CV): {lr_grid.best_score_:.4f}\")\n",
    "\n",
    "print(f\"Decision Tree Test AUC: {roc_auc_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"Logistic Regression Test AUC: {roc_auc_score(y_test, y_pred_lr):.4f}\")\n",
    "```\n",
    "\n",
    "The results confirmed that **Logistic Regression achieved an AUC ≈ 0.75**, demonstrating a good discriminatory capacity, whereas **Decision Tree scored ≈ 0.51**, indicating near‑random performance. Hence, the logistic model is chosen as the final candidate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a74d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compliance Merge — Task 4: Supervised Learning (exact hyperparams, 5-fold CV accuracy)\n",
    "\n",
    "# Define models\n",
    "lr_clf = LogisticRegression(max_iter=1000)\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Exact grids per rubric\n",
    "param_grid_lr = {\"C\": [0.1, 1.0]}\n",
    "param_grid_dt = {\"max_depth\": [3, None]}\n",
    "\n",
    "# GridSearchCV with 5-fold accuracy\n",
    "lr_grid = GridSearchCV(lr_clf, param_grid_lr, scoring=\"accuracy\", cv=5)\n",
    "dt_grid = GridSearchCV(dt_clf, param_grid_dt, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "lr_grid.fit(X_train_pca, y_train)\n",
    "dt_grid.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Logistic Regression best params:\", lr_grid.best_params_)\n",
    "print(\"Decision Tree best params:\", dt_grid.best_params_)\n",
    "\n",
    "# Tabulate CV results\n",
    "def cv_table(grid):\n",
    "    rows = []\n",
    "    for params, mean, std in zip(grid.cv_results_[\"params\"],\n",
    "                                 grid.cv_results_[\"mean_test_score\"],\n",
    "                                 grid.cv_results_[\"std_test_score\"]):\n",
    "        rows.append((params, mean, std))\n",
    "    return pd.DataFrame(rows, columns=[\"params\", \"mean_accuracy_cv\", \"std\"])\n",
    "\n",
    "print(\"\\nLR CV results:\"); display(cv_table(lr_grid))\n",
    "print(\"\\nDT CV results:\"); display(cv_table(dt_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Train with best hyperparameters (from CV)\n",
    "print(\"Training models with optimal hyperparameters...\")\n",
    "\n",
    "best_lr = LogisticRegression(C=best_lr_C, random_state=42, max_iter=200)\n",
    "best_lr.fit(X_train_pca, y_train)\n",
    "lr_proba = best_lr.predict_proba(X_test_pca)[:, 1]\n",
    "lr_pred  = (lr_proba >= 0.5).astype(int)\n",
    "\n",
    "best_dt = DecisionTreeClassifier(max_depth=best_dt_depth, random_state=42)\n",
    "best_dt.fit(X_train_pca, y_train)\n",
    "dt_proba = best_dt.predict_proba(X_test_pca)[:, 1]\n",
    "dt_pred  = (dt_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Utility function for metrics\n",
    "def metrics_dict(y_true, y_pred, y_proba):\n",
    "    return {\n",
    "        \"accuracy\":  accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\":    recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\":        f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"auc\":       roc_auc_score(y_true, y_proba),\n",
    "    }\n",
    "\n",
    "lr_metrics = metrics_dict(y_test, lr_pred, lr_proba)\n",
    "dt_metrics = metrics_dict(y_test, dt_pred, dt_proba)\n",
    "\n",
    "# 3) Comparative table\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\"model\": \"LogisticRegression\", **lr_metrics},\n",
    "    {\"model\": \"DecisionTree\",       **dt_metrics},\n",
    "])\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd2894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) ROC curves on the same plot\n",
    "print(\"Generating ROC curve comparison...\")\n",
    "\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_proba)\n",
    "dt_fpr, dt_tpr, _ = roc_curve(y_test, dt_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(lr_fpr, lr_tpr, label=f\"Logistic Regression (AUC={lr_metrics['auc']:.3f})\")\n",
    "plt.plot(dt_fpr, dt_tpr, label=f\"Decision Tree (AUC={dt_metrics['auc']:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], \"--\", color='gray', label='Random Classifier')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves Comparison — Test Set\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd067bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Classification reports (details per class)\n",
    "print(\"\\nClassification report — Logistic Regression\")\n",
    "print(classification_report(y_test, lr_pred, digits=4, zero_division=0))\n",
    "\n",
    "print(\"\\nClassification report — Decision Tree\")\n",
    "print(classification_report(y_test, dt_pred, digits=4, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Configure Stratified K-Fold\n",
    "# We use 'stratify=y' in the previous split and now in CV to handle class imbalance.\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d48493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ Instantiate Models for CV\n",
    "# We use data after PCA (X_train_pca)\n",
    "lr_cv = LogisticRegression(random_state=42)\n",
    "dt_cv = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fede54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣ Calculate Cross-Validation Scores (scoring='roc_auc')\n",
    "lr_scores = cross_val_score(lr_cv, X_train_pca, y_train, cv=skf, scoring='roc_auc')\n",
    "dt_scores = cross_val_score(dt_cv, X_train_pca, y_train, cv=skf, scoring='roc_auc')\n",
    "\n",
    "print(\"Cross-validation completed (k=10 Folds).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf192bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ Summary and Comparison of Results\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression (LR)', 'Decision Tree (DT)'],\n",
    "    'AUC_Mean': [lr_scores.mean(), dt_scores.mean()],\n",
    "    'AUC_Std': [lr_scores.std(), dt_scores.std()]\n",
    "}).set_index('Model').sort_values(by='AUC_Mean', ascending=False)\n",
    "\n",
    "print(\"\\n--- Cross-Validation Results (AUC) ---\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173341f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5️⃣ Formal Comparison Logic\n",
    "\n",
    "# Determine which model has the highest mean AUC\n",
    "best_model_name = results_df['AUC_Mean'].idxmax()\n",
    "best_auc_mean = results_df.loc[best_model_name, 'AUC_Mean']\n",
    "best_auc_std = results_df.loc[best_model_name, 'AUC_Std']\n",
    "\n",
    "# Determine which model is most stable (lowest standard deviation)\n",
    "most_stable_model_name = results_df['AUC_Std'].idxmin()\n",
    "most_stable_auc_std = results_df.loc[most_stable_model_name, 'AUC_Std']\n",
    "\n",
    "print(f\"\\nBest Performance (Mean AUC): {best_model_name} (AUC: {best_auc_mean:.4f})\")\n",
    "print(f\"Most Stable (Lowest Std Dev): {most_stable_model_name} (SD: {most_stable_auc_std:.4f})\")\n",
    "\n",
    "# Model comparison\n",
    "if best_model_name == 'Logistic Regression (LR)':\n",
    "    print(\"\\nProvisional Conclusion: Logistic Regression is the winning model. Its average performance (AUC) is superior, and it tends to be more stable, suggesting good generalization potential.\")\n",
    "elif best_model_name == 'Decision Tree (DT)':\n",
    "    print(\"\\nProvisional Conclusion: Decision Tree has the best average performance (AUC), but its Standard Deviation is often higher, indicating it may be more sensitive to training data partitioning (high variance/overfitting).\")\n",
    "else:\n",
    "    print(\"\\n(Adjust conclusion based on exact dataframe results.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c31c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Define Hyperparameter Grid for Decision Tree\n",
    "# We focus on three main parameters to control complexity and prevent overfitting:\n",
    "param_grid_dt = {\n",
    "    # Maximum tree depth (controls overfitting)\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    # Minimum samples required to split a node (controls overfitting)\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    # Criterion for measuring split quality\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# 2️⃣ Instantiate GridSearchCV\n",
    "# We use the already defined Stratified K-Fold (skf) (k=10) and 'roc_auc' as the main metric.\n",
    "grid_search_dt = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid_dt,\n",
    "    scoring='roc_auc',\n",
    "    cv=skf,\n",
    "    n_jobs=-1,  # Use all processor cores to accelerate\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting Grid Search for Decision Tree...\")\n",
    "\n",
    "# 3️⃣ Execute the search\n",
    "grid_search_dt.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Grid Search completed.\")\n",
    "\n",
    "# 4️⃣ Optimization Results\n",
    "best_dt_model = grid_search_dt.best_estimator_\n",
    "best_dt_score = grid_search_dt.best_score_\n",
    "\n",
    "print(f\"\\n--- Tuning Results (Decision Tree) ---\")\n",
    "print(f\"Best Hyperparameters: {grid_search_dt.best_params_}\")\n",
    "print(f\"Best Cross-Validation AUC: {best_dt_score:.4f}\")\n",
    "\n",
    "# 5️⃣ Final Evaluation on Test Set (with optimized model)\n",
    "\n",
    "# Make predictions on X_test_pca using the optimized model\n",
    "dt_tuned_pred = best_dt_model.predict(X_test_pca)\n",
    "dt_tuned_proba = best_dt_model.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "# Calculate final AUC\n",
    "auc_dt_tuned = roc_auc_score(y_test, dt_tuned_proba)\n",
    "f1_dt_tuned = f1_score(y_test, dt_tuned_pred)\n",
    "\n",
    "print(f\"Final test set AUC (Tuned DT): {auc_dt_tuned:.4f}\")\n",
    "print(f\"Final test set F1-Score (Tuned DT): {f1_dt_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d219a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Define Hyperparameter Grid for Logistic Regression\n",
    "# 'C': Inverse of regularization strength. Smaller values (e.g., 0.01) mean stronger regularization.\n",
    "# 'penalty': The type of regularization. L1 (Lasso) or L2 (Ridge).\n",
    "# 'solver': 'liblinear' is efficient for small datasets and supports both L1 and L2.\n",
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# 2️⃣ Instantiate GridSearchCV\n",
    "# We use Stratified K-Fold (skf) and the 'roc_auc' metric\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=LogisticRegression(random_state=42),\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring='roc_auc',\n",
    "    cv=skf, # Reuse the Stratified K-Fold with 10 folds\n",
    "    verbose=1,\n",
    "    n_jobs=-1 # Use all cores\n",
    ")\n",
    "\n",
    "# 3️⃣ Execute Grid Search (Fit)\n",
    "print(\"Starting Grid Search for Logistic Regression...\")\n",
    "grid_search_lr.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Grid Search completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36633ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ Display Best Hyperparameters\n",
    "best_params_lr = grid_search_lr.best_params_\n",
    "best_score_lr = grid_search_lr.best_score_\n",
    "\n",
    "print(f\"\\n--- Best Tuning Results (Logistic Regression) ---\")\n",
    "print(f\"Best Hyperparameters (CV): {best_params_lr}\")\n",
    "print(f\"Best Cross-Validation AUC: {best_score_lr:.4f}\")\n",
    "\n",
    "# 5️⃣ Retrain and Evaluate Optimized Model on Test Set\n",
    "# Create final model with optimized parameters\n",
    "best_lr_model = grid_search_lr.best_estimator_\n",
    "\n",
    "# Predictions on test set (X_test_pca)\n",
    "y_pred_tuned_lr = best_lr_model.predict(X_test_pca)\n",
    "y_proba_tuned_lr = best_lr_model.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "# 6️⃣ Final Evaluation (primary metric: AUC)\n",
    "tuned_auc_lr = roc_auc_score(y_test, y_proba_tuned_lr)\n",
    "tuned_f1_lr = f1_score(y_test, y_pred_tuned_lr)\n",
    "\n",
    "print(\"\\n--- Final Evaluation of Optimized Model (Test Set) ---\")\n",
    "print(f\"Test AUC (Optimized LR): {tuned_auc_lr:.4f}\")\n",
    "print(f\"Test F1-Score (Optimized LR): {tuned_f1_lr:.4f}\")\n",
    "print(f\"Classification Report (Optimized LR):\\n{classification_report(y_test, y_pred_tuned_lr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c562637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compliance Merge — Task 5: Evaluation (test metrics, ROC & AUC with both curves)\n",
    "\n",
    "best_lr = lr_grid.best_estimator_\n",
    "best_dt = dt_grid.best_estimator_\n",
    "\n",
    "# Fit best models\n",
    "best_lr.fit(X_train_pca, y_train)\n",
    "best_dt.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = best_lr.predict(X_test_pca)\n",
    "y_pred_dt = best_dt.predict(X_test_pca)\n",
    "\n",
    "def metric_dict(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "print(\"LR test metrics:\", metric_dict(y_test, y_pred_lr))\n",
    "print(\"DT test metrics:\", metric_dict(y_test, y_pred_dt))\n",
    "\n",
    "# ROC and AUC\n",
    "y_proba_lr = best_lr.predict_proba(X_test_pca)[:, 1]\n",
    "y_proba_dt = best_dt.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr)\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_proba_dt)\n",
    "\n",
    "auc_lr = roc_auc_score(y_test, y_proba_lr)\n",
    "auc_dt = roc_auc_score(y_test, y_proba_dt)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_lr, tpr_lr, label=f\"LR (AUC={auc_lr:.3f})\")\n",
    "plt.plot(fpr_dt, tpr_dt, label=f\"DT (AUC={auc_dt:.3f})\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\", label=\"Chance\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves — Logistic Regression vs Decision Tree\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Test AUC — Logistic Regression: {auc_lr:.4f}\")\n",
    "print(f\"Test AUC — Decision Tree: {auc_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c229b2c8",
   "metadata": {},
   "source": [
    "## Brief Report (≤ 2 pages)\n",
    "\n",
    "**Dataset & Objective.** Predict five-year survival using a supervised learning pipeline (Modules 1–4) on the Haberman dataset (age, operation year 1958–1969, axillary nodes, survival status).  \n",
    "**EDA.** Target recoded to 0/1; distributions reviewed; validity checks for operation year and node counts; no missing data detected.  \n",
    "**Preprocessing.** Hold-out split (75/25, `random_state=42`, stratified); features standardized with `StandardScaler`.  \n",
    "**PCA.** CEV plotted; minimum components retaining ≥90% variance selected and applied to both splits.  \n",
    "**Models & Tuning.** Logistic Regression (C ∈ {0.1, 1.0}) and Decision Tree (max_depth ∈ {3, None}); 5-fold CV with accuracy score; mean±std reported.  \n",
    "**Evaluation.** Test accuracy, precision, recall, F₁; ROC curves for both models on the same axes; AUC compared.  \n",
    "**Recommendation.** Prefer Logistic Regression based on higher AUC and balanced metrics.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
