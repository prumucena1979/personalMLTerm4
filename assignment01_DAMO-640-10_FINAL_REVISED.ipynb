{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70bf2033",
   "metadata": {},
   "source": [
    "# DAMO-640-10 · Fall 2025  \n",
    "## Assignment 1 — Supervised Learning (Haberman’s Survival Dataset)\n",
    "\n",
    "**Group 03:** \n",
    "**Katragadda, Jayasri |**\n",
    "**Oshiro, Renato Hiroyuki |**\n",
    "**Pemmasani, Sridevi |**\n",
    "**Prumucena, Fabio**\n",
    "**Course:** Advanced Data Analytics (DAMO-640)  \n",
    "**Institution:** University of Niagara Falls Canada  \n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "This notebook presents the full implementation of a supervised learning pipeline applied to the *Haberman’s Survival Dataset* (Haberman, 1976). The objective is to build, tune, and evaluate predictive models capable of estimating post-surgery survival outcomes in breast cancer patients. The methodology strictly follows the assignment requirements, covering data loading, preprocessing, dimensionality reduction (PCA), hyperparameter tuning via cross-validation, and final model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c67f6",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_curve, roc_auc_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set a fixed random state for reproducibility\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1acb189",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploratory Data Analysis (EDA)\n",
    "\n",
    "The dataset is loaded from the UCI repository, and the four columns are assigned descriptive names: `age`, `operation_year`, `axillary_nodes`, and `survival_status`. The initial EDA phase focuses on verifying data integrity, examining feature distributions, and assessing the balance of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a52c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Data Loading & EDA\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\"\n",
    "column_names = [\"age\", \"operation_year\", \"axillary_nodes\", \"survival_status\"]\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n",
    "print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "# Display first five rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Check for missing or invalid values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check class distribution\n",
    "class_ratios = df['survival_status'].value_counts(normalize=True).round(4)\n",
    "print(\"\\nClass Distribution (1 = survived \\u2265 5 years, 2 = died < 5 years):\\n\")\n",
    "print(class_ratios)\n",
    "\n",
    "print(\"\\nEDA Summary: No missing values detected. Target variable shows a significant class imbalance (73.5%% survived vs. 26.5%% died).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c1b189",
   "metadata": {},
   "source": [
    "## 3. Preprocessing and Data Split\n",
    "\n",
    "This stage prepares the data for model training. The target variable is re-encoded to a binary format ($\\text{Survived}=1, \\text{Died}=0$), and the features are standardized using `StandardScaler` to ensure all variables contribute equally to the model training process. A 75\\% train / 25\\% test split is performed, stratified by the target variable to maintain the class ratio in both subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Preprocessing\n",
    "\n",
    "# 1. Target Encoding: Convert survival_status (1/2) to binary label (1/0)\n",
    "# {1: 1 (Survived), 2: 0 (Died)}\n",
    "df['label'] = df['survival_status'].map({1: 1, 2: 0})\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df[[\"age\", \"operation_year\", \"axillary_nodes\"]]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# 2. Data Split: 75% Train / 25% Test (stratified, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "print(f\"Data split: Training set size {X_train.shape[0]}, Test set size {X_test.shape[0]}\")\n",
    "\n",
    "# 3. Feature Standardization: Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature standardization completed. Features are now centered and scaled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7c5d4",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction: Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is applied to the standardized training features to reduce dimensionality. The goal is to select the smallest number of principal components (PCs) that collectively explain at least 90\\% of the total variance, thereby creating a more compact feature representation for the subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7336d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Dimensionality Reduction\n",
    "\n",
    "# Apply PCA to the standardized training data to find explained variance\n",
    "pca = PCA()\n",
    "pca.fit(X_train_std)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Determine the number of components for >= 90% variance\n",
    "# np.searchsorted finds the index where 0.90 should be inserted to maintain order\n",
    "n_components = np.searchsorted(cumulative_variance, 0.90) + 1\n",
    "retained_variance = cumulative_variance[n_components - 1]\n",
    "\n",
    "print(f\"Individual explained variance ratios: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative explained variance: {cumulative_variance}\")\n",
    "print(f\"Result: {n_components} components retain {retained_variance:.4f} (≥ 90%) of the variance.\")\n",
    "\n",
    "# Enhanced cumulative explained variance plot for full rubric coverage\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Main cumulative variance plot\n",
    "ax1.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, \n",
    "         marker='o', linestyle='--', linewidth=2, markersize=6, color='steelblue')\n",
    "ax1.axhline(y=0.90, color='r', linestyle='-', linewidth=2, label='90% Variance Threshold')\n",
    "ax1.axvline(x=n_components, color='g', linestyle='--', linewidth=2, \n",
    "           label=f'{n_components} Components Selected')\n",
    "\n",
    "# Add percentage annotations for key points\n",
    "for i, var in enumerate(cumulative_variance):\n",
    "    if i == n_components - 1:  # Selected component\n",
    "        ax1.annotate(f'{var:.3f}', (i+1, var), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center', fontweight='bold', color='green')\n",
    "\n",
    "ax1.set_title('Cumulative Explained Variance vs. Number of Principal Components')\n",
    "ax1.set_xlabel('Number of Components')\n",
    "ax1.set_ylabel('Cumulative Explained Variance')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# Individual variance contribution plot\n",
    "bars = ax2.bar(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "               pca.explained_variance_ratio_, alpha=0.7, color='coral')\n",
    "ax2.set_title('Individual Principal Component Variance Contribution')\n",
    "ax2.set_xlabel('Principal Component')\n",
    "ax2.set_ylabel('Explained Variance Ratio')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight selected components\n",
    "for i in range(n_components):\n",
    "    ax2.bar(i+1, pca.explained_variance_ratio_[i], alpha=0.9, color='darkgreen')\n",
    "\n",
    "# Add value labels on bars (positioned in the middle with white font)\n",
    "for i, (bar, value) in enumerate(zip(bars, pca.explained_variance_ratio_)):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height/2,\n",
    "             f'{value:.3f}\\n({value*100:.1f}%)', \n",
    "             ha='center', va='center', fontsize=9, fontweight='bold', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis for comprehensive coverage\n",
    "print(f\"\\n--- Detailed PCA Analysis ---\")\n",
    "print(f\"Total features in original dataset: {X_train_std.shape[1]}\")\n",
    "print(f\"Selected principal components: {n_components}\")\n",
    "print(f\"Dimensionality reduction: {X_train_std.shape[1]} → {n_components}\")\n",
    "print(f\"Variance retained: {retained_variance:.4f} ({retained_variance*100:.2f}%)\")\n",
    "print(f\"Variance explained by each selected component:\")\n",
    "for i in range(n_components):\n",
    "    print(f\"  PC{i+1}: {pca.explained_variance_ratio_[i]:.4f} ({pca.explained_variance_ratio_[i]*100:.2f}%)\")\n",
    "\n",
    "# Transform data using the selected number of components\n",
    "pca_final = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_train_pca = pca_final.fit_transform(X_train_std)\n",
    "X_test_pca = pca_final.transform(X_test_std)\n",
    "\n",
    "print(f\"\\nData successfully transformed to {X_train_pca.shape[1]} dimensions.\")\n",
    "print(f\"Training set shape after PCA: {X_train_pca.shape}\")\n",
    "print(f\"Test set shape after PCA: {X_test_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11238461",
   "metadata": {},
   "source": [
    "## 5. Supervised Learning and Hyperparameter Tuning (Cross-Validation)\n",
    "\n",
    "Two classifiers, **Logistic Regression (LR)** and **Decision Tree (DT)**, are trained on the PCA-transformed training data. Hyperparameter tuning is performed using 5-fold cross-validation (CV) on the training set, with accuracy as the scoring metric, to select the optimal model settings as specified by the assignment rubric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a74d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Supervised Learning and Hyperparameter Tuning\n",
    "\n",
    "# Define models\n",
    "lr_clf = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "dt_clf = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# Define hyperparameter grids\n",
    "param_grid_lr = {\"C\": [0.1, 1.0]}\n",
    "param_grid_dt = {\"max_depth\": [3, None]}\n",
    "\n",
    "# Use GridSearchCV for 5-fold cross-validation with accuracy scoring\n",
    "lr_grid = GridSearchCV(lr_clf, param_grid_lr, scoring=\"accuracy\", cv=5)\n",
    "dt_grid = GridSearchCV(dt_clf, param_grid_dt, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "# Fit models on PCA-transformed training data\n",
    "lr_grid.fit(X_train_pca, y_train)\n",
    "dt_grid.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"--- Cross-Validation Results ---\")\n",
    "print(f\"Logistic Regression Best Params: {lr_grid.best_params_} (Accuracy: {lr_grid.best_score_:.4f})\")\n",
    "print(f\"Decision Tree Best Params: {dt_grid.best_params_} (Accuracy: {dt_grid.best_score_:.4f})\")\n",
    "\n",
    "# Function to display CV results concisely\n",
    "def display_cv_results(grid, model_name, param_key):\n",
    "    results = []\n",
    "    for params, mean, std in zip(grid.cv_results_[\"params\"],\n",
    "                                 grid.cv_results_[\"mean_test_score\"],\n",
    "                                 grid.cv_results_[\"std_test_score\"]):\n",
    "        param_value = params[param_key]\n",
    "        results.append({f'{model_name} Parameter ({param_key})': str(param_value), 'Mean CV Accuracy': f'{mean:.4f}', 'Std Dev': f'{std:.4f}'})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nLogistic Regression CV Table:\")\n",
    "display(display_cv_results(lr_grid, 'LR', 'C'))\n",
    "\n",
    "print(\"\\nDecision Tree CV Table:\")\n",
    "display(display_cv_results(dt_grid, 'DT', 'max_depth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4d5b2",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Comparative Analysis\n",
    "\n",
    "The optimally tuned models are evaluated on the independent hold-out test set (Task 5). Performance is assessed using a comprehensive set of metrics, with a focus on the Area Under the Receiver Operating Characteristic (ROC) Curve (AUC) to determine the best model for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Model Evaluation\n",
    "\n",
    "# Retrain models with optimal hyperparameters (best_estimator_ from GridSearchCV)\n",
    "best_lr = lr_grid.best_estimator_\n",
    "best_dt = dt_grid.best_estimator_\n",
    "\n",
    "# Predict probabilities and classes on the PCA-transformed test set\n",
    "lr_proba = best_lr.predict_proba(X_test_pca)[:, 1]\n",
    "dt_proba = best_dt.predict_proba(X_test_pca)[:, 1]\n",
    "lr_pred = best_lr.predict(X_test_pca)\n",
    "dt_pred = best_dt.predict(X_test_pca)\n",
    "\n",
    "# Utility function for metrics\n",
    "def metrics_dict(y_true, y_pred, y_proba):\n",
    "    return {\n",
    "        \"Accuracy\":  accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\":    recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1-Score\":  f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"AUC\":       roc_auc_score(y_true, y_proba),\n",
    "    }\n",
    "\n",
    "lr_metrics = metrics_dict(y_test, lr_pred, lr_proba)\n",
    "dt_metrics = metrics_dict(y_test, dt_pred, dt_proba)\n",
    "\n",
    "# Comparative table\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\"Model\": \"Logistic Regression\", **lr_metrics},\n",
    "    {\"Model\": \"Decision Tree\",       **dt_metrics},\n",
    "])\n",
    "metrics_df = metrics_df.set_index('Model').round(4)\n",
    "\n",
    "print(\"Test Set Performance Comparison (Optimal Models):\")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd2894-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curves\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_proba)\n",
    "dt_fpr, dt_tpr, _ = roc_curve(y_test, dt_proba)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(lr_fpr, lr_tpr, label=f'Logistic Regression (AUC = {lr_metrics[\"AUC\"]:.4f})')\n",
    "plt.plot(dt_fpr, dt_tpr, label=f'Decision Tree (AUC = {dt_metrics[\"AUC\"]:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.50)')\n",
    "\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve Comparison')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_discussion",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Recommendation\n",
    "The comparative analysis indicates that the **Logistic Regression** model is superior for this dataset. With an AUC of $0.743$, the LR model demonstrates good discriminatory power, significantly outperforming the Decision Tree model, which achieved an AUC of $0.526$ (near-random performance). This suggests that the relationship between the principal components and the survival outcome is predominantly linear, which the Decision Tree failed to capture effectively without overfitting. Given its superior performance, robustness, and inherent interpretability as a linear model, **Logistic Regression is the recommended final model** for predicting post-surgery survival outcomes based on the Haberman's Survival Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 8. Model Comparison Based on F1-Score\n",
    "# ============================================\n",
    "\n",
    "# Create a focused F1-Score comparison\n",
    "f1_comparison = comparison[['F1']].copy()\n",
    "f1_comparison = f1_comparison.sort_values(\"F1\", ascending=False)\n",
    "\n",
    "# Create a more detailed F1-Score visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart for F1-Score\n",
    "bars = ax1.barh(f1_comparison.index, f1_comparison[\"F1\"], \n",
    "                alpha=0.8, color=[\"darkgreen\", \"darkred\"])\n",
    "ax1.set_xlabel(\"F1-Score\")\n",
    "ax1.set_title(\"Model Comparison: F1-Score Performance\")\n",
    "ax1.grid(axis=\"x\", alpha=0.3)\n",
    "ax1.set_xlim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(f1_comparison[\"F1\"]):\n",
    "    ax1.text(v + 0.01, i, f'{v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "# Radar chart showing all metrics\n",
    "metrics_for_radar = comparison[['Accuracy', 'F1', 'AUC']].values\n",
    "model_names = comparison.index.tolist()\n",
    "\n",
    "# Number of metrics\n",
    "num_metrics = len(['Accuracy', 'F1', 'AUC'])\n",
    "angles = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "ax2 = plt.subplot(122, projection='polar')\n",
    "colors = ['steelblue', 'coral']\n",
    "\n",
    "for i, model in enumerate(model_names):\n",
    "    values = metrics_for_radar[i].tolist()\n",
    "    values += values[:1]  # Complete the circle\n",
    "    \n",
    "    ax2.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i])\n",
    "    ax2.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(['Accuracy', 'F1', 'AUC'])\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title(\"Multi-Metric Performance Radar Chart\", pad=20)\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print F1-Score focused summary\n",
    "print(\"F1-Score Performance Ranking:\")\n",
    "for i, (model, f1_score) in enumerate(f1_comparison.iterrows(), 1):\n",
    "    print(f\"{i}. {model}: {f1_score['F1']:.4f}\")\n",
    "\n",
    "print(f\"\\nF1-Score Analysis:\")\n",
    "print(f\"Best F1-Score: {f1_comparison.iloc[0].name} ({f1_comparison.iloc[0]['F1']:.4f})\")\n",
    "print(f\"F1-Score Difference: {f1_comparison.iloc[0]['F1'] - f1_comparison.iloc[1]['F1']:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "best_f1_model = f1_comparison.iloc[0].name\n",
    "if f1_comparison.iloc[0]['F1'] > 0.6:\n",
    "    print(f\"\\n✅ {best_f1_model} shows good F1-Score performance (>0.6), indicating balanced precision and recall.\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Both models show moderate F1-Score performance (<0.6), suggesting room for improvement in precision-recall balance.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
